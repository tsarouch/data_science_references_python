{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "In this toy example, \n",
    "we want to see what are the baselines of prediction accuracy based on extremes scenarios of input signals.\n",
    "\n",
    "We study two distinct cases:\n",
    "a) The input is just noise\n",
    "b) The input hides a clear signan (smoking affects men and die while not affects women)\n",
    "\n",
    "We want to see how the model reacts on such conditions\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_features_values_combinations(list_a, list_b):\n",
    "    \"\"\" Returns a list of combinations of the values of \n",
    "    e.g. from the lists \n",
    "    list_a = ['L', 'M', 'W']\n",
    "    list_b = ['F', 'I', 'S']\n",
    "    we get the combinations:\n",
    "    [('L', 'F'), ('L', 'I'), ('L', 'S'), ('M', 'F'), ('M', 'I') ...\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    return list(itertools.product(list_a, list_b))\n",
    "\n",
    "def generate_dataset_with_probabilities(n_datapoints,\n",
    "                                        x_values,\n",
    "                                        y_values,\n",
    "                                        y_probabilites=None):\n",
    "    \"\"\" Return a dataset of given possible values with given possible probabilities\n",
    "    :n_datapoints: The number of datapoints we want to generate\n",
    "    :x_values: 1-D array e.g. ['Man', 'Woman']\n",
    "    :y_values: 1-D array e.g. ['meat', 'fish', 'vegetables']\n",
    "    :y_probabilites: 1-D array-like e.g. [0.5, 0.25, 0.25]\n",
    "        The probabilities associated with each entry in entries_values.\n",
    "        If not given the sample assumes a uniform distribution over all entries_values\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    datapoints = []\n",
    "    for i in range(n_datapoints):\n",
    "        datapoint = []\n",
    "        datapoint = (x_values,) + (np.random.choice(y_values, p=y_probabilites),)\n",
    "        datapoints.append(datapoint)\n",
    "        \n",
    "    return datapoints\n",
    "\n",
    "\n",
    "def generate_datapoints(n_datapoints, var_x, var_y, case_probabilites):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Generate the datapoints according to the desired destribution\n",
    "    datapoints = []\n",
    "    all_cases = get_features_values_combinations(var_x, var_y)\n",
    "    n_cases = len(all_cases)\n",
    "    n_datapoints_per_case = int(1. * n_datapoints / n_cases)\n",
    "    for case in all_cases:\n",
    "        sex = case[:-1][0]\n",
    "        case_probabilites = cases_probabilites[sex]\n",
    "        datapoints.extend(\n",
    "        generate_dataset_with_probabilities(n_datapoints_per_case,\n",
    "                                            sex, \n",
    "                                            var_y,\n",
    "                                            y_probabilites=case_probabilites))\n",
    "    N_datapoints = len(datapoints)\n",
    "    print \"All the combinations we can have are: \", all_cases\n",
    "    print \"N total datapoints: \", N_datapoints\n",
    "    return np.array(datapoints)\n",
    "\n",
    "\n",
    "def get_transformed_dataframe(df):\n",
    "    \"\"\" Transform the DataFrame accordinlgy (dummy vars / encoding)\n",
    "    \"\"\"\n",
    "    # And transform the categorical variables\n",
    "    from sklearn import preprocessing\n",
    "    df_transformed_X = pd.get_dummies(df[var_x_name])\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    encoder.fit(df[var_y_name].values)       \n",
    "    df_transformed_y = pd.DataFrame(data = encoder.transform(df[var_y_name].values), columns=[var_y_name])\n",
    "\n",
    "    df_transformed = pd.concat([df_transformed_X, df_transformed_y], axis=1)\n",
    "    df_transformed.head(2)\n",
    "    return df_transformed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Independent Variable\n",
    "var_x_name = 'sex'\n",
    "var_x = ['Man', 'Woman']\n",
    "\n",
    "# Dependent Variable\n",
    "var_y_name = 'status'\n",
    "var_y = ['Died', 'Alive']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a) Example with All probabilities 50 - 50  (input signal = noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the combinations we can have are:  [('Man', 'Died'), ('Man', 'Alive'), ('Woman', 'Died'), ('Woman', 'Alive')]\n",
      "N total datapoints:  1000\n",
      "Frequency of  ('Man', 'Died') :  0.238\n",
      "Frequency of  ('Man', 'Alive') :  0.262\n",
      "Frequency of  ('Woman', 'Died') :  0.236\n",
      "Frequency of  ('Woman', 'Alive') :  0.264\n",
      "LogisticRegression score: 0.518182\n"
     ]
    }
   ],
   "source": [
    "# We assume that we do not have any strong signal in our input\n",
    "# e.g. for men smoking or not 50% diy\n",
    "# the same for women\n",
    "cases_probabilites = {\n",
    "    'Man' : [0.5, 0.5],\n",
    "    'Woman' : [0.5, 0.5]}\n",
    "datapoints = generate_datapoints(1000, var_x, var_y, case_probabilites)\n",
    "\n",
    "# Cross check that the datapoints we generated follow the frequency we thought\n",
    "all_cases = get_features_values_combinations(var_x, var_y)\n",
    "for case in all_cases:\n",
    "    print \"Frequency of \", case, \": \", 1. * len([x for x in datapoints if tuple(x.tolist()) == case]) / len(datapoints)\n",
    "    \n",
    "df = pd.DataFrame(data = datapoints, columns=[var_x_name, var_y_name])\n",
    "\n",
    "df_transformed = get_transformed_dataframe(df)\n",
    "\n",
    "# Split our sample to train / set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df_transformed[['Man', 'Woman']].values\n",
    "y = df_transformed['status'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "# Apply model\n",
    "from sklearn import  linear_model\n",
    "logistic = linear_model.LogisticRegression()\n",
    "print('LogisticRegression score: %f'\n",
    "      % logistic.fit(X_train, y_train).score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) Example with some clear input singnal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the combinations we can have are:  [('Man', 'Died'), ('Man', 'Alive'), ('Woman', 'Died'), ('Woman', 'Alive')]\n",
      "N total datapoints:  1000\n",
      "Frequency of  ('Man', 'Died') :  0.452\n",
      "Frequency of  ('Man', 'Alive') :  0.048\n",
      "Frequency of  ('Woman', 'Died') :  0.049\n",
      "Frequency of  ('Woman', 'Alive') :  0.451\n",
      "LogisticRegression score: 0.912121\n"
     ]
    }
   ],
   "source": [
    "# Here we assume that Men are affected a lot from smoking 90% die while women not, only 10% die\n",
    "cases_probabilites = {\n",
    "    'Man' : [0.9, 0.1],\n",
    "    'Woman' : [0.1, 0.9]}\n",
    "datapoints = generate_datapoints(1000, var_x, var_y, case_probabilites)\n",
    "\n",
    "# Cross check that the datapoints we generated follow the frequency we thought\n",
    "all_cases = get_features_values_combinations(var_x, var_y)\n",
    "for case in all_cases:\n",
    "    print \"Frequency of \", case, \": \", 1. * len([x for x in datapoints if tuple(x.tolist()) == case]) / len(datapoints)\n",
    "    \n",
    "df = pd.DataFrame(data = datapoints, columns=[var_x_name, var_y_name])\n",
    "\n",
    "df_transformed = get_transformed_dataframe(df)\n",
    "\n",
    "# Split our sample to train / set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df_transformed[['Man', 'Woman']].values\n",
    "y = df_transformed['status'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "# Apply model\n",
    "from sklearn import  linear_model\n",
    "logistic = linear_model.LogisticRegression()\n",
    "print('LogisticRegression score: %f'\n",
    "      % logistic.fit(X_train, y_train).score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example with some clear input singnal half noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We assume that we do not have any strong signal in our input\n",
    "# e.g. for men smoking or not 50% diy\n",
    "# the same for women\n",
    "cases_probabilites = {\n",
    "    'Man' : [0.5, 0.5],\n",
    "    'Woman' : [0.5, 0.5]}\n",
    "datapoints = generate_datapoints(1000, var_x, var_y, case_probabilites)\n",
    "\n",
    "# Cross check that the datapoints we generated follow the frequency we thought\n",
    "all_cases = get_features_values_combinations(var_x, var_y)\n",
    "for case in all_cases:\n",
    "    print \"Frequency of \", case, \": \", 1. * len([x for x in datapoints if tuple(x.tolist()) == case]) / len(datapoints)\n",
    "    \n",
    "df = pd.DataFrame(data = datapoints, columns=[var_x_name, var_y_name])\n",
    "\n",
    "df_transformed = get_transformed_dataframe(df)\n",
    "\n",
    "# Split our sample to train / set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df_transformed[['Man', 'Woman']].values\n",
    "y = df_transformed['status'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "# Apply model\n",
    "from sklearn import  linear_model\n",
    "logistic = linear_model.LogisticRegression()\n",
    "print('LogisticRegression score: %f'\n",
    "      % logistic.fit(X_train, y_train).score(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We assume that there are 3 differnt values for dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the combinations we can have are:  [('Man', 'Died'), ('Man', 'Alive'), ('Man', 'Almost_dead'), ('Woman', 'Died'), ('Woman', 'Alive'), ('Woman', 'Almost_dead')]\n",
      "N total datapoints:  9996\n",
      "Frequency of  ('Man', 'Died') :  0.164865946379\n",
      "Frequency of  ('Man', 'Alive') :  0.168867547019\n",
      "Frequency of  ('Man', 'Almost_dead') :  0.166266506603\n",
      "Frequency of  ('Woman', 'Died') :  0.163865546218\n",
      "Frequency of  ('Woman', 'Alive') :  0.170668267307\n",
      "Frequency of  ('Woman', 'Almost_dead') :  0.165466186475\n",
      "LogisticRegression score: 0.330706\n"
     ]
    }
   ],
   "source": [
    "var_y = ['Died', 'Alive', 'Almost_dead']\n",
    "\n",
    "cases_probabilites = {\n",
    "    'Man' : [0.333, 0.333, 1-0.333-.333],\n",
    "    'Woman' : [0.333, 0.333, 1-.333-.333]}\n",
    "\n",
    "\n",
    "datapoints = generate_datapoints(10000, var_x, var_y, case_probabilites)\n",
    "\n",
    "# Cross check that the datapoints we generated follow the frequency we thought\n",
    "all_cases = get_features_values_combinations(var_x, var_y)\n",
    "for case in all_cases:\n",
    "    print \"Frequency of \", case, \": \", 1. * len([x for x in datapoints if tuple(x.tolist()) == case]) / len(datapoints)\n",
    "    \n",
    "df = pd.DataFrame(data = datapoints, columns=[var_x_name, var_y_name])\n",
    "\n",
    "df_transformed = get_transformed_dataframe(df)\n",
    "\n",
    "# Split our sample to train / set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df_transformed[['Man', 'Woman']].values\n",
    "y = df_transformed['status'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "# Apply model\n",
    "from sklearn import  linear_model\n",
    "logistic = linear_model.LogisticRegression()\n",
    "print('LogisticRegression score: %f'\n",
    "      % logistic.fit(X_train, y_train).score(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We have 4 categories for dependent var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the combinations we can have are:  [('Man', 'Died'), ('Man', 'Alive'), ('Man', 'Almost_dead'), ('Man', 'Almost_alive'), ('Woman', 'Died'), ('Woman', 'Alive'), ('Woman', 'Almost_dead'), ('Woman', 'Almost_alive')]\n",
      "N total datapoints:  10000\n",
      "Frequency of  ('Man', 'Died') :  0.1279\n",
      "Frequency of  ('Man', 'Alive') :  0.1239\n",
      "Frequency of  ('Man', 'Almost_dead') :  0.1237\n",
      "Frequency of  ('Man', 'Almost_alive') :  0.1245\n",
      "Frequency of  ('Woman', 'Died') :  0.1265\n",
      "Frequency of  ('Woman', 'Alive') :  0.121\n",
      "Frequency of  ('Woman', 'Almost_dead') :  0.1263\n",
      "Frequency of  ('Woman', 'Almost_alive') :  0.1262\n",
      "LogisticRegression score: 0.245455\n"
     ]
    }
   ],
   "source": [
    "var_y = ['Died', 'Alive', 'Almost_dead', 'Almost_alive']\n",
    "\n",
    "cases_probabilites = {\n",
    "    'Man' : [0.25, 0.25, 0.25, 0.25],\n",
    "    'Woman' : [0.25, 0.25, 0.25, 0.25]\n",
    "}\n",
    "\n",
    "\n",
    "datapoints = generate_datapoints(10000, var_x, var_y, case_probabilites)\n",
    "\n",
    "# Cross check that the datapoints we generated follow the frequency we thought\n",
    "all_cases = get_features_values_combinations(var_x, var_y)\n",
    "for case in all_cases:\n",
    "    print \"Frequency of \", case, \": \", 1. * len([x for x in datapoints if tuple(x.tolist()) == case]) / len(datapoints)\n",
    "    \n",
    "df = pd.DataFrame(data = datapoints, columns=[var_x_name, var_y_name])\n",
    "\n",
    "df_transformed = get_transformed_dataframe(df)\n",
    "\n",
    "# Split our sample to train / set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df_transformed[['Man', 'Woman']].values\n",
    "y = df_transformed['status'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "# Apply model\n",
    "from sklearn import  linear_model\n",
    "logistic = linear_model.LogisticRegression()\n",
    "print('LogisticRegression score: %f'\n",
    "      % logistic.fit(X_train, y_train).score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Point 1 \n",
    "```\n",
    "IF the input data is pure noise, the best accuracy the model can get is just 1/(N classes) (N classe = the number of values of the dependent variable)\n",
    "\n",
    "So if we have 2 cases of output (Alive, Died) the prediction accuracy can be 1/2 0.5\n",
    "So if we have 3 cases of output (Alive, Died, Almost_alive) the prediction accuracy can be 1/3 = 0.3\n",
    "So if we have 4 cases of output (Alive, Died, Almost_alive) the prediction accuracy can be 1/4 = 0.25\n",
    "\n",
    "One the same time, with the very same model, if we have data that come from some signal (eg. death beacause of smoking does depends on the sex) then the model accordinly can reach better accuracy levels.\n",
    "```\n",
    "\n",
    "# Point 2\n",
    "```\n",
    "If we deal with categorical variables, it is important to check if any of the values of the independent variables carries information. E.g. although we have a single variable Sex, maybe one component of the sex - Men is noise (so that 50% are affected of smoking and other 50% is not affected) while the other component - Women hold some clear signal 90% affected and 10% not affected. In such scenario the prediction accuracy will lie between the max (90% when both keep clear signal 90-10) and min (50% where both are jus noise 50-50). Actually this number proven above to be 70%. \n",
    "```\n",
    "\n",
    "# Point 3\n",
    "```\n",
    "In order to judge about how well our model is working based on prediction accuracy, we have always to keep in mind that even for a perfect model (witout any bugs or bad tunings) , there is a max of prediction accuracy can be reached dictated by the signal of the data themselves.  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
