{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem Description\n",
    "\n",
    "We assume that we deal with a problem of e-commerce platform.\n",
    "People come from different hardware profiles (Linux, Mac, Firefox, I.E. etc)\n",
    "and navigate to different categories.\n",
    "\n",
    "We want to predict the preference of categories based on the user hardware profile\n",
    "\n",
    "Features dealing with the hardware profile (The independent variables):\n",
    "e.g.\n",
    "platform: mobile, tablet, desktop , unknown ...\n",
    "os: Windows, Windows 98, Bada, Solaris, Firefox OS, Ubuntu, OpenBSD  ...\n",
    "browser : Safari, BlackBerry WebKit, Pinterest, NetFront, PhantomJS, Chrome Mobile iOS, Chromium, Opera ...                 \n",
    "device : Samsung SM-A300FU, HUAWEI LYO-L21, YD201, HTC One M9_Prime Camera Edit, Samsung SM-N910H                          \n",
    "\n",
    "And the dependent variable:\n",
    "e.g.\n",
    "category: electronics, toys, pills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !!! Relevant reading\n",
    "# http://blog.yhat.com/posts/logistic-regression-and-python.html\n",
    "# http://stats.stackexchange.com/questions/224051/one-hot-vs-dummy-encoding-in-scikit-learn\n",
    "# http://blog.yhat.com/posts/logistic-regression-python-rodeo.html    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>os</th>\n",
       "      <th>browser</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L</td>\n",
       "      <td>F</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  os browser category\n",
       "0  L       F        E\n",
       "1  L       F        T"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we keep only the initial letter of the strings for simplicity\n",
    "\n",
    "# Independent variables\n",
    "os_values = ['L', 'M', 'W']\n",
    "browser_values = ['F', 'I', 'S']\n",
    "# Dependent Variable\n",
    "categories = ['E', 'T', 'P']\n",
    "\n",
    "# The input with hand so to check if our model works \n",
    "# The categories for the combinations not found below simply get equal weights\n",
    "features_categories_probabilites = {\n",
    "    ('L', 'F') : [0.7, 0.2, 0.1],\n",
    "    ('M', 'S') : [0.8, 0.1, 0.1],\n",
    "    ('W', 'I') : [0.1, 0.1, 0.8]\n",
    "}\n",
    "\n",
    "def generate_dataset_with_probabilities(n_datapoints,\n",
    "                                     input_values,\n",
    "                                     input_probabilites=None):\n",
    "    \"\"\" Return a dataset of given possible values with given possible probabilities\n",
    "    :n_datapoints: The number of datapoints we want to generate\n",
    "    :input_values: 1-D array e.g. ['meat', 'fish', 'vegetables']\n",
    "    :input_probabilites: 1-D array-like e.g. [0.5, 0.25, 0.25]\n",
    "        The probabilities associated with each entry in entries_values.\n",
    "        If not given the sample assumes a uniform distribution over all entries_values\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    datapoints = []\n",
    "    for i in range(n_datapoints):\n",
    "        datapoints.append(\n",
    "            np.random.choice(input_values, \n",
    "                             p=input_probabilites))\n",
    "    return datapoints\n",
    "\n",
    "\n",
    "def get_features_values_combinations(list_a, list_b):\n",
    "    \"\"\" Returns a list of combinations of the values of \n",
    "    e.g. from the lists \n",
    "    list_a = ['L', 'M', 'W']\n",
    "    list_b = ['F', 'I', 'S']\n",
    "    we get the combinations:\n",
    "    [('L', 'F'), ('L', 'I'), ('L', 'S'), ('M', 'F'), ('M', 'I') ...\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    return list(itertools.product(list_a, list_b))\n",
    "\n",
    "\n",
    "n_datapoints = 100\n",
    "datapoints = []\n",
    "features_combinations = get_features_values_combinations(os_values, browser_values)\n",
    "n_datapoints_per_combination = int (1. * n_datapoints / len(features_combinations)) \n",
    "for feature_comb in features_combinations:\n",
    "    features_cat_prob = features_categories_probabilites.get(feature_comb, [0.33, 0.33, 1-0.33-0.33])\n",
    "    for ii in range(n_datapoints_per_combination):\n",
    "        generated_categories = generate_dataset_with_probabilities(\n",
    "            n_datapoints_per_combination,\n",
    "            categories,\n",
    "            input_probabilites=features_cat_prob)                \n",
    "        datapoints.extend([[feature_comb[0], feature_comb[1], c] for c in generated_categories])\n",
    "    \n",
    "\n",
    "    \n",
    "# Construct the final dataframe\n",
    "x_vars = ['os', 'browser']\n",
    "y_var = 'category'\n",
    "\n",
    "columns.append(y_var)\n",
    "\n",
    "df_data = pd.DataFrame(data=datapoints, columns=x_vars + [y_var])\n",
    "df_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df_data[x_vars].values\n",
    "y = df_data[y_var].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# And convert to DataFrames\n",
    "df_train_X = pd.DataFrame(data=X_train, columns=x_vars)\n",
    "df_train_y = pd.DataFrame(data=y_train, columns=[y_var])\n",
    "\n",
    "df_test_X = pd.DataFrame(data=X_test, columns=x_vars)\n",
    "df_test_y = pd.DataFrame(data=y_test, columns=[y_var])\n",
    "\n",
    "df_train = pd.concat([df_train_X, df_train_y], axis=1)\n",
    "df_test = pd.concat([df_test_X, df_test_y], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: I",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-941b3d6e0339>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlogistic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m print('LogisticRegression score: %f'\n\u001b[0;32m----> 4\u001b[0;31m       % logistic.fit(X_train, y_train).score(X_test, y_test))\n\u001b[0m",
      "\u001b[0;32m/Users/charilaostsarouchas/anaconda/lib/python2.7/site-packages/sklearn/linear_model/logistic.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=np.float64,\n\u001b[0;32m-> 1173\u001b[0;31m                          order=\"C\")\n\u001b[0m\u001b[1;32m   1174\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/charilaostsarouchas/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    519\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    520\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[1;32m    522\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m/Users/charilaostsarouchas/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    380\u001b[0m                                       force_all_finite)\n\u001b[1;32m    381\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: I"
     ]
    }
   ],
   "source": [
    "from sklearn import  linear_model\n",
    "logistic = linear_model.LogisticRegression()\n",
    "print('LogisticRegression score: %f'\n",
    "      % logistic.fit(X_train, y_train).score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def feature_tranformation_chain(df, \n",
    "                           y_column=None,\n",
    "                           vectorizer=None, \n",
    "                           encoder=None):\n",
    "    \"\"\" Transforms a dataframe to a form accepted by Logistic Regression\n",
    "    :y_column: The column of the dependent variable\n",
    "    TODO encoder looks like is not needed as arg, xcheck\n",
    "    \"\"\"\n",
    "    from sklearn.feature_extraction import DictVectorizer\n",
    "    from sklearn import preprocessing\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    df_transformed = df.copy()\n",
    "    x_columns = df_transformed.columns.values.tolist() \n",
    "    x_columns = [x for x in x_columns if x != y_column]\n",
    "\n",
    "    df_transformed_X = df_transformed[x_columns]\n",
    "    # Take vectorize the categorical independent variables\n",
    "    if not vectorizer:\n",
    "        vectorizer = DictVectorizer(sparse=False) \n",
    "        df_transformed_X = vectorizer.fit_transform(df_transformed_X.to_dict(orient='records'))\n",
    "    else:\n",
    "        df_transformed_X = vectorizer.transform(df_transformed_X.to_dict(orient='records'))\n",
    "    original_columns = x_columns\n",
    "    transformed_x_columns = vectorizer.feature_names_\n",
    "\n",
    "\n",
    "\n",
    "    y_col_idx_map = None\n",
    "    if y_column:\n",
    "        df_transformed_Y = df_transformed[[y_column]]\n",
    "\n",
    "        # Encode the categorical dependent variable\n",
    "        encoder = preprocessing.LabelEncoder()\n",
    "        encoder.fit(df_transformed_Y[y_column].values)\n",
    "        encoded_labels = encoder.transform(df_transformed_Y[y_column].values)\n",
    "        df_transformed_Y[y_column] = encoded_labels\n",
    "        y_col_idx_map = dict(zip(df_transformed_Y[y_column].values, \n",
    "                                       encoder.inverse_transform(df_transformed_Y[y_column].values)))\n",
    "\n",
    "    \n",
    "\n",
    "    # TODO check that next step is not needed\n",
    "    \"\"\"\n",
    "    # 2. Concert to numeric the independent variables\n",
    "    #print df_transformed\n",
    "    df_transformed_X = df_transformed_X.convert_objects(convert_numeric=True)\n",
    "    #print df_transformed\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \"\"\" TODO cross check that we do not need the part below\n",
    "    # 4. Exclude the baseline columns for each categorical variable separately to avoid collinearity\n",
    "    #e.g.\n",
    "    #original_columns = ['os', 'browser']\n",
    "    #transformed_columns = ['browser=F', 'browser=I', 'browser=S', 'category', 'os=L', 'os=M', 'os=W']\n",
    "    #calculated baseline_columns = ['os=W', 'browser=S']\n",
    "    #baseline_columns_idx = [6, 2]\n",
    "    \n",
    "        \n",
    "    print transformed_columns\n",
    "\n",
    "    baseline_columns = []    \n",
    "    for original_column in original_columns:\n",
    "        baseline_columns.append([c for c in transformed_columns if original_column + '=' in c][-1])\n",
    "    baseline_columns_idxs = [transformed_columns.index(i) for i in baseline_columns]\n",
    "    print baseline_columns_idxs\n",
    "    df_transformed = np.delete(df_transformed, baseline_columns_idxs, 1) \n",
    "    transformed_columns = np.delete(transformed_columns, baseline_columns_idxs).tolist() \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Concert to DataFrames (and concat with dependent component if exists)\n",
    "    df_transformed_X = pd.DataFrame(data = df_transformed_X, columns=transformed_x_columns)\n",
    "    if y_column:\n",
    "        df_transformed_Y = pd.DataFrame(data = df_transformed_Y, columns=[y_column])\n",
    "        df_transformed = pd.concat([df_transformed_X, df_transformed_Y], axis=1)\n",
    "    else:\n",
    "        df_transformed = pd.DataFrame(data = df_transformed_X, columns=transformed_x_columns)\n",
    "\n",
    "\n",
    "\n",
    "    return df_transformed, vectorizer, encoder, y_col_idx_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charilaostsarouchas/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# Output\n",
    "df_train_transformed, vectorizer, encoder , y_col_idx_map = feature_tranformation_chain(df_train, \n",
    "                                                                                  y_column=y_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>browser=F</th>\n",
       "      <th>browser=I</th>\n",
       "      <th>browser=S</th>\n",
       "      <th>os=L</th>\n",
       "      <th>os=M</th>\n",
       "      <th>os=W</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   browser=F  browser=I  browser=S  os=L  os=M  os=W  category\n",
       "0        0.0        1.0        0.0   0.0   1.0   0.0         1\n",
       "1        0.0        0.0        1.0   0.0   1.0   0.0         0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transformed.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_logistic_regresion_model(df, y_column):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    x_columns = df.columns.values.tolist() \n",
    "    x_columns.remove(y_column)\n",
    "    \n",
    "    from sklearn import linear_model, datasets\n",
    "    logreg = linear_model.LogisticRegression(C=1e5)\n",
    "    logreg.fit(df[x_columns].values, df[y_column].values)\n",
    "    return logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logistic_regresion_model = get_logistic_regresion_model(df_train_transformed, 'category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charilaostsarouchas/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "df_test_transformed, vectorizer, encoder , _ = feature_tranformation_chain(df_test, y_column=y_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test = df_test_transformed[y_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_predict = logistic_regresion_model.predict(df_test_transformed.drop([y_var],axis=1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_predicted = pd.DataFrame(data=zip(y_test, y_predict), columns=['real', 'prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_predicted['cor'] = df_predicted.apply(lambda x: x['real']==x['prediction'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42777777777777776"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.* df_predicted['cor'].sum() / len(df_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-240368289bd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogistic_regresion_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test_transformed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "logistic_regresion_model.predict_proba(df_test_transformed.values[i]).tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.448848567463995, 0.25489236990701236, 0.29625906262899265], [0.2813156019776315, 0.4494225116631866, 0.2692618863591819], [0.35131137131936746, 0.37016849583776545, 0.27852013284286703]]\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.DataFrame(\n",
    "    data=[\n",
    "   ['L', 'F'],\n",
    "   ['L', 'I'],\n",
    "   ['W', 'S']\n",
    "   ], \n",
    "columns=['os', 'browser'])\n",
    "\n",
    "# 1. Transform input data\n",
    "df_test_transformed, vectorizer, encoder, _ = feature_tranformation_chain(df_test,  vectorizer = vectorizer)\n",
    "df_test_transformed\n",
    "\n",
    "probs_all = []\n",
    "for i in range(len(df_test_transformed)):\n",
    "    probs = logistic_regresion_model.predict_proba(df_test_transformed.values[i]).tolist()[0]\n",
    "    probs_all.append(probs)\n",
    "#data = []\n",
    "#for i in range(9):\n",
    "#    data.append([y_col_idx_map[i], probs[i]])   \n",
    "#print \"~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\"\n",
    "#    return pd.DataFrame(data=data, \n",
    "#                        columns=['category'+df_suffix, 'probability'+df_suffix])\n",
    "\n",
    "print probs_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#y_col_idx_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove colums generated by the transformation\n",
    "# TODO next drop with hand make it more robust\n",
    "#df_test_transformed = df_test_transformed.drop('category', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.44884857,  0.25489237,  0.29625906]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regresion_model.predict_proba(df_test_transformed.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic_regresion_model.predict_proba(df_test_transformed.values[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
